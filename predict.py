import pandas as pd
import numpy as np
import extinction
from astropy.cosmology import Planck18 as cosmo
from astropy import units as u
import joblib
import os
from tqdm import tqdm
from scipy.optimize import curve_fit
from scipy.stats import theilslopes
import warnings
warnings.filterwarnings('ignore')

print("=== D·ª± ƒëo√°n t·∫≠p test TDE ===")

# C·∫•u h√¨nh tham s·ªë
BASE_PATH = './data/'
EFF_WAVELENGTHS = {'u': 3641, 'g': 4704, 'r': 6155, 'i': 7504, 'z': 8695, 'y': 10056}
R_V = 3.1
FILTERS = ['u', 'g', 'r', 'i', 'z', 'y']

# 1. T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
print("--- ƒêang t·∫£i m√¥ h√¨nh ---")
model_info = joblib.load('optimized_tde_model.pkl')
selected_features = model_info['features']
best_threshold = model_info['best_threshold']
models = model_info['models']

print(f"Lo·∫°i m√¥ h√¨nh: {model_info['strategy']}")
print(f"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng: {len(selected_features)}")
print(f"Ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.3f}")
print(f"S·ªë m√¥ h√¨nh: {len(models)}")

# 2. T·∫£i metadata c·ªßa t·∫≠p test
print("\n--- ƒêang t·∫£i d·ªØ li·ªáu t·∫≠p test ---")
test_log = pd.read_csv(f'{BASE_PATH}test_log.csv')
print(f"S·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng trong t·∫≠p test: {len(test_log)}")

# 3. H√†m feature engineering cho t·∫≠p test (gi·ªØ nh·∫•t qu√°n v·ªõi t·∫≠p train)
def tde_decay_model(t, alpha, t0, A):
    return A * (t - t0 + 1e-6) ** (-alpha)

def fit_decay_alpha(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == len(f) - 1: return np.nan
    t_decay = t[peak_idx:] - t[peak_idx]
    f_decay = f[peak_idx:]
    try:
        popt, _ = curve_fit(
            tde_decay_model, t_decay, f_decay,
            p0=[1.5, 0, f[peak_idx]],
            bounds=([0.5, -10, 0], [3.0, 10, np.inf]),
            maxfev=500
        )
        alpha = popt[0]
        return alpha if 0.5 <= alpha <= 3.0 else np.nan
    except:
        return np.nan

def is_single_peak(group):
    f = group['flux_corrected'].values
    if len(f) < 3: return 0
    peaks = (f[1:-1] > f[:-2]) & (f[1:-1] > f[2:])
    return int(np.sum(peaks) == 1)

def rise_fall_ratio(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == 0 or peak_idx == len(f) - 1: return np.nan
    rise_time = t[peak_idx] - t[0]
    fall_time = t[-1] - t[peak_idx]
    return rise_time / (fall_time + 1e-6)

def extract_improved_features(df):
    """Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng c·∫£i ti·∫øn v√† ·ªïn ƒë·ªãnh (gi·ªëng t·∫≠p train)"""
    features_list = []
    
    for object_id in df['object_id'].unique():
        obj_data = df[df['object_id'] == object_id].sort_values('mjd')
        
        features = {}
        flux = obj_data['flux_corrected'].values
        mjd = obj_data['mjd'].values
        
        # 1. X·ª≠ l√Ω flux √¢m
        flux_positive = flux.clip(min=1e-9)
        flux_abs = np.abs(flux)
        
        # ƒê·∫∑c tr∆∞ng theo ph√¢n v·ªã
        for q in [10, 25, 50, 75, 90]:
            features[f'flux_q{q}'] = np.percentile(flux_positive, q)
            features[f'flux_abs_q{q}'] = np.percentile(flux_abs, q)
        
        features['positive_ratio'] = (flux > 0).mean()
        features['flux_abs_median'] = np.median(flux_abs)
        
        # 2. ƒê·∫∑c tr∆∞ng kho·∫£ng c√°ch th·ªùi gian quan s√°t
        if len(mjd) > 1:
            mjd_diff = np.diff(mjd)
            features['gap_mean'] = np.mean(mjd_diff)
            features['gap_std'] = np.std(mjd_diff)
            features['gap_max'] = np.max(mjd_diff)
            features['gap_median'] = np.median(mjd_diff)
            features['obs_density'] = len(obj_data) / (mjd[-1] - mjd[0] + 1e-9)
        
        # 3. ƒê·∫∑c tr∆∞ng theo d·∫£i s√≥ng
        unique_filters = obj_data['Filter'].nunique()
        features['filter_coverage'] = unique_filters / 6.0
        
        # T·ª∑ l·ªá s·ªë quan s√°t theo t·ª´ng d·∫£i
        for band in ['u', 'g', 'r', 'i', 'z', 'y']:
            band_count = (obj_data['Filter'] == band).sum()
            features[f'{band}_obs_ratio'] = band_count / len(obj_data)
        
        # 4. ƒê·∫∑c tr∆∞ng t√≠n hi·ªáu tr√™n nhi·ªÖu
        snr = flux_positive / (obj_data['flux_err'].values + 1e-9)
        features['snr_median'] = np.median(snr)
        features['snr_q10'] = np.percentile(snr, 10)
        features['snr_q90'] = np.percentile(snr, 90)
        
        # 5. ƒê·ªô ·ªïn ƒë·ªãnh xu h∆∞·ªõng (Theil-Sen)
        if len(flux) >= 5:
            try:
                x = np.arange(len(flux))
                slope, _, _, _ = theilslopes(flux, x)
                features['trend_slope'] = slope
                # ƒê·ªô ·ªïn ƒë·ªãnh xu h∆∞·ªõng: ƒë·ªô l·ªõn t∆∞∆°ng ƒë·ªëi c·ªßa ph·∫ßn d∆∞
                predicted = slope * x + np.median(flux)
                residuals = flux - predicted
                features['trend_stability'] = 1.0 / (np.std(residuals) / (np.std(flux) + 1e-9) + 1e-9)
            except:
                features['trend_slope'] = 0
                features['trend_stability'] = 0
        else:
            features['trend_slope'] = 0
            features['trend_stability'] = 0
        
        # 6. ƒê·∫∑c tr∆∞ng h√¨nh th√°i chu·ªói th·ªùi gian t·ªïng qu√°t (kh√¥ng ph·ª• thu·ªôc nh√£n)
        if len(flux) >= 3:
            # Ch·ªâ s·ªë bi·∫øn thi√™n
            features['variability_index'] = np.std(flux) / (np.mean(flux_positive) + 1e-9)
            
            # ƒê·ªô s·∫Øc ƒë·ªânh
            features['peakiness'] = np.max(flux_positive) / (np.median(flux_positive) + 1e-9)
            
            # ƒê·ªô b·∫•t ƒë·ªëi x·ª©ng
            peak_idx = np.argmax(flux)
            if 0 < peak_idx < len(flux) - 1:
                before_peak = flux[:peak_idx]
                after_peak = flux[peak_idx+1:]
                std_before = np.std(before_peak) if len(before_peak) > 1 else 0
                std_after = np.std(after_peak) if len(after_peak) > 1 else 0
                features['asymmetry'] = std_after / (std_before + 1e-9)
            else:
                features['asymmetry'] = 0
            
            # T·ª∑ l·ªá tƒÉng/gi·∫£m to√†n c·ª•c
            if len(flux) > 5:
                first_half = flux[:len(flux)//2]
                second_half = flux[len(flux)//2:]
                features['rise_fall_ratio_global'] = np.mean(first_half) / (np.mean(second_half) + 1e-9)
        
        features['object_id'] = object_id
        features_list.append(features)
    
    return pd.DataFrame(features_list)

# 4. X·ª≠ l√Ω light curve c·ªßa t·∫≠p test
def process_test_lightcurves():
    """X·ª≠ l√Ω light curve t·∫≠p test v√† tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng"""
    
    print("\n--- ƒêang x·ª≠ l√Ω light curve t·∫≠p test ---")
    
    # Ki·ªÉm tra c·∫•u tr√∫c file t·∫≠p test
    test_lc_paths = []
    
    # Ki·ªÉm tra file test ƒë∆°n
    single_test_path = f'{BASE_PATH}test_lightcurves/test_full_lightcurves.csv'
    if os.path.exists(single_test_path):
        test_lc_paths = [single_test_path]
        print("ƒê√£ t√¨m th·∫•y file test ƒë∆°n")
    else:
        # Ki·ªÉm tra c√°c file test chia nh·ªè
        for i in range(1, 21):
            split_path = f'{BASE_PATH}split_{i:02d}/test_full_lightcurves.csv'
            if os.path.exists(split_path):
                test_lc_paths.append(split_path)
        print(f"T√¨m th·∫•y {len(test_lc_paths)} file test ph√¢n m·∫£nh")
    
    if not test_lc_paths:
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file light curve c·ªßa t·∫≠p test")
    
    all_test_features = []
    
    for lc_path in tqdm(test_lc_paths, desc="X·ª≠ l√Ω c√°c ph√¢n m·∫£nh test"):
        # T·∫£i d·ªØ li·ªáu light curve
        lc = pd.read_csv(lc_path)
        lc.rename(columns={'Time (MJD)': 'mjd', 'Flux': 'flux', 'Flux_err': 'flux_err'}, inplace=True)
        
        # Ch·ªâ x·ª≠ l√Ω object_id t·ªìn t·∫°i trong test_log
        relevant_objects = test_log['object_id'].unique()
        lc = lc[lc['object_id'].isin(relevant_objects)]
        
        if lc.empty:
            continue
        
        processed_dfs = []
        
        # Hi·ªáu ch·ªânh flux cho t·ª´ng object
        for object_id in lc['object_id'].unique():
            object_lc = lc[lc['object_id'] == object_id].copy()
            if object_lc.empty:
                continue
                
            # L·∫•y gi√° tr·ªã EBV c·ªßa object
            object_ebv = test_log[test_log['object_id'] == object_id]['EBV'].iloc[0]
            A_v = R_V * object_ebv
            
            flux_corrected_list = []
            for _, row in object_lc.iterrows():
                A_lambda = extinction.fitzpatrick99(
                    np.array([EFF_WAVELENGTHS[row['Filter']]]), A_v, R_V
                )[0]
                flux_corrected_list.append(row['flux'] * 10**(0.4 * A_lambda))
            
            object_lc['flux_corrected'] = flux_corrected_list
            processed_dfs.append(object_lc)
        
        if not processed_dfs:
            continue
            
        df = pd.concat(processed_dfs)
        
        # T√≠nh kho·∫£ng c√°ch v√† ƒë·ªô s√°ng tuy·ªát ƒë·ªëi (d√πng Z c·ªßa t·∫≠p test, c√≥ th·ªÉ c√≥ sai s·ªë)
        df['Z'] = df['object_id'].map(test_log.set_index('object_id')['Z'])
        z_values = df['Z'].fillna(0).values
        z_values[z_values <= 0] = 1e-6
        
        # T√≠nh kho·∫£ng c√°ch
        dist_pc = cosmo.luminosity_distance(z_values).to(u.pc).value
        df['distance_pc'] = dist_pc
        
        # T√≠nh ƒë·ªô s√°ng tuy·ªát ƒë·ªëi
        df['flux_positive'] = df['flux_corrected'].clip(lower=1e-9)
        df['apparent_mag'] = -2.5 * np.log10(df['flux_positive'])
        df['absolute_mag'] = df['apparent_mag'] - 5 * (np.log10(df['distance_pc']) - 1)
        
        # T√≠nh to√°n c∆° b·∫£n
        df['snr'] = df['flux_corrected'] / df['flux_err']
        df = df.sort_values(['object_id', 'mjd'])
        
        # Feature engineering
        grouped = df.groupby('object_id')
        
        # ƒê·∫∑c tr∆∞ng c∆° b·∫£n
        agg_features = grouped.agg(
            flux_mean=('flux_corrected', 'mean'),
            flux_std=('flux_corrected', 'std'),
            flux_max=('flux_corrected', 'max'),
            flux_min=('flux_corrected', 'min'),
            flux_skew=('flux_corrected', 'skew'),
            mjd_span=('mjd', lambda x: x.max() - x.min()),
            snr_mean=('snr', 'mean'),
            snr_max=('snr', 'max'),
            snr_std=('snr', 'std'),
            obs_count=('mjd', 'size')
        )
        
        # ƒê·∫∑c tr∆∞ng ƒë·ªô s√°ng tuy·ªát ƒë·ªëi
        abs_mag_agg = grouped.agg(
            abs_mag_min=('absolute_mag', 'min'),
            abs_mag_max=('absolute_mag', 'max'),
            abs_mag_mean=('absolute_mag', 'mean'),
            abs_mag_std=('absolute_mag', 'std'),
            abs_mag_span=('absolute_mag', lambda x: x.max() - x.min())
        )
        agg_features = agg_features.join(abs_mag_agg, how='left')
        
        # ƒê·ªô d·ªëc c√≥ tr·ªçng s·ªë theo th·ªùi gian
        def weighted_slope(group):
            group = group.sort_values('mjd')
            delta_mjd = np.diff(group['mjd'])
            delta_flux = np.diff(group['flux_corrected'])
            slopes = delta_flux / (delta_mjd + 1e-9)
            return np.mean(slopes) if len(slopes) > 0 else 0
        
        agg_features['weighted_slope_mean'] = grouped.apply(weighted_slope)
        
        # ƒê·∫∑c tr∆∞ng v·∫≠t l√Ω
        agg_features['decay_alpha'] = grouped.apply(fit_decay_alpha)
        agg_features['is_single_peak'] = grouped.apply(is_single_peak)
        agg_features['rise_fall_ratio'] = grouped.apply(rise_fall_ratio)
        
        # ƒê·∫∑c tr∆∞ng theo d·∫£i
        pivot_mean = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='mean').add_suffix('_mean')
        pivot_std = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='std').add_suffix('_std')
        pivot_max = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='max').add_suffix('_max')
        agg_features = agg_features.join([pivot_mean, pivot_std, pivot_max], how='left')
        
        # ƒê·∫∑c tr∆∞ng m√†u s·∫Øc
        for f in FILTERS:
            if f'{f}_mean' not in agg_features.columns: 
                agg_features[f'{f}_mean'] = np.nan
        agg_features['color_g_r_mean'] = agg_features['g_mean'] - agg_features['r_mean']
        agg_features['color_r_i_mean'] = agg_features['r_mean'] - agg_features['i_mean']
        agg_features['color_i_z_mean'] = agg_features['i_mean'] - agg_features['z_mean']
        
        # Th√™m c√°c ƒë·∫∑c tr∆∞ng c·∫£i ti·∫øn
        improved_features = extract_improved_features(df)
        agg_features = agg_features.merge(improved_features, on='object_id', how='left')
        
        all_test_features.append(agg_features)
    
    if not all_test_features:
        raise ValueError("Kh√¥ng t·∫°o ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng cho t·∫≠p test")
    
    # G·ªôp t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng
    test_feature_df = pd.concat(all_test_features)
    test_feature_df.reset_index(inplace=True)
    
    return test_feature_df

# 5. Sinh ƒë·∫∑c tr∆∞ng cho t·∫≠p test
test_features_df = process_test_lightcurves()

# 6. G·ªôp metadata v√† ƒë·∫∑c tr∆∞ng
print("\n--- G·ªôp d·ªØ li·ªáu t·∫≠p test ---")
test_final = test_log.merge(test_features_df, on='object_id', how='left')

# Ki·ªÉm tra ƒë·∫∑c tr∆∞ng b·ªã thi·∫øu
missing_features = set(selected_features) - set(test_final.columns)
if missing_features:
    print(f"C·∫£nh b√°o: thi·∫øu {len(missing_features)} ƒë·∫∑c tr∆∞ng, s·∫Ω ƒëi·ªÅn 0")
    for feature in missing_features:
        test_final[feature] = 0

# 7. Chu·∫©n b·ªã d·ªØ li·ªáu d·ª± ƒëo√°n
X_test = test_final[selected_features].copy()

# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (theo chi·∫øn l∆∞·ª£c c·ªßa t·∫≠p train)
numeric_cols = X_test.select_dtypes(include=[np.number]).columns
X_test[numeric_cols] = X_test[numeric_cols].fillna(0)

std_cols = [c for c in X_test.columns if 'std' in c]
X_test[std_cols] = X_test[std_cols].fillna(0)

print(f"K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng t·∫≠p test: {X_test.shape}")

# 8. Ti·∫øn h√†nh d·ª± ƒëo√°n
print("\n--- ƒêang d·ª± ƒëo√°n ---")
test_predictions = []

for i, model in enumerate(models):
    pred = model.predict_proba(X_test)[:, 1]
    test_predictions.append(pred)
    print(f"M√¥ h√¨nh {i+1} ƒë√£ d·ª± ƒëo√°n xong")

# Trung b√¨nh d·ª± ƒëo√°n
test_preds_mean = np.mean(test_predictions, axis=0)

# √Åp d·ª•ng ng∆∞·ª°ng t·ªëi ∆∞u
test_binary = (test_preds_mean >= best_threshold).astype(int)

print(f"D·ª± ƒëo√°n t·∫≠p test ho√†n t·∫•t:")
print(f"X√°c su·∫•t d·ª± ƒëo√°n trung b√¨nh: {test_preds_mean.mean():.4f} ¬± {test_preds_mean.std():.4f}")
print(f"T·ª∑ l·ªá m·∫´u d∆∞∆°ng ƒë∆∞·ª£c d·ª± ƒëo√°n: {test_binary.mean():.4f}")
print(f"Ph√¢n b·ªë x√°c su·∫•t d·ª± ƒëo√°n:")
for percentile in [10, 25, 50, 75, 90]:
    value = np.percentile(test_preds_mean, percentile)
    print(f"  Ph√¢n v·ªã {percentile}%: {value:.4f}")

# 9. T·∫°o file submission
print("\n--- T·∫°o file submission ---")
submission = pd.DataFrame({
    'object_id': test_final['object_id'],
    'predicted': test_binary
})

# ƒê·∫£m b·∫£o m·ªçi object trong t·∫≠p test ƒë·ªÅu c√≥ d·ª± ƒëo√°n
missing_objects = set(test_log['object_id']) - set(submission['object_id'])
if missing_objects:
    print(f"C·∫£nh b√°o: {len(missing_objects)} object ch∆∞a c√≥ d·ª± ƒëo√°n, g√°n 0")
    missing_df = pd.DataFrame({
        'object_id': list(missing_objects),
        'predicted': 0
    })
    submission = pd.concat([submission, missing_df], ignore_index=True)

# S·∫Øp x·∫øp theo object_id
submission = submission.sort_values('object_id')

# L∆∞u file submission
submission_file = 'submission.csv'
submission.to_csv(submission_file, index=False)

print(f"‚úÖ ƒê√£ t·∫°o file submission: '{submission_file}'")
print(f"K√≠ch th∆∞·ªõc file submission: {submission.shape}")
print(f"S·ªë m·∫´u d∆∞∆°ng ƒë∆∞·ª£c d·ª± ƒëo√°n: {submission['predicted'].sum()}")
print(f"T·ª∑ l·ªá m·∫´u d∆∞∆°ng: {submission['predicted'].mean():.4f}")

# 10. L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n chi ti·∫øt
detailed_predictions = pd.DataFrame({
    'object_id': test_final['object_id'],
    'prediction_prob': test_preds_mean,
    'predicted': test_binary
})
detailed_predictions.to_csv('test_detailed_predictions.csv', index=False)
print("ƒê√£ l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n chi ti·∫øt: 'test_detailed_predictions.csv'")

# 11. Ph√¢n t√≠ch d·ª± ƒëo√°n
print("\n--- Ph√¢n t√≠ch d·ª± ƒëo√°n ---")
print(f"Ng∆∞·ª°ng s·ª≠ d·ª•ng: {best_threshold}")
print(f"Kho·∫£ng x√°c su·∫•t d·ª± ƒëo√°n: [{test_preds_mean.min():.4f}, {test_preds_mean.max():.4f}]")
print(f"M·∫´u d∆∞∆°ng ƒë·ªô tin c·∫≠y cao (p > 0.7): {(test_preds_mean > 0.7).sum()}")
print(f"M·∫´u √¢m ƒë·ªô tin c·∫≠y cao (p < 0.3): {(test_preds_mean < 0.3).sum()}")

# Ki·ªÉm tra ƒë·ªô nh·∫•t qu√°n c·ªßa ƒë·ªô quan tr·ªçng ƒë·∫∑c tr∆∞ng
if hasattr(models[0], 'feature_importances_'):
    feature_importance = np.mean([model.feature_importances_ for model in models], axis=0)
    importance_df = pd.DataFrame({
        'feature': selected_features,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t tr√™n t·∫≠p test:")
    print(importance_df.head(10))

print("\nüéâ Ho√†n t·∫•t d·ª± ƒëo√°n t·∫≠p test!")
print("üì§ H√£y n·ªôp file 'submission.csv' l√™n Kaggle")
print(f"üìä ƒêi·ªÉm LB k·ª≥ v·ªçng x·∫•p x·ªâ: {model_info['oof_score']:.4f} (d·ª±a tr√™n OOF F1)")
print("üí° N·∫øu ƒëi·ªÉm LB kh√¥ng t·ªët, h√£y ki·ªÉm tra s·ª± kh√°c bi·ªát ph√¢n ph·ªëi gi·ªØa train v√† test")
