import pandas as pd
import numpy as np
import extinction
from astropy.cosmology import Planck18 as cosmo
from astropy import units as u
import joblib
import os
from tqdm import tqdm
from scipy.optimize import curve_fit
from scipy.stats import theilslopes
import warnings
from scipy.signal import find_peaks
warnings.filterwarnings('ignore')

print("=== D·ª∞ ƒêO√ÅN T·∫¨P TEST TDE V·ªöI ELAsTiCC2 MODEL ===")

# C·∫•u h√¨nh tham s·ªë
BASE_PATH = '/content/data/'
EFF_WAVELENGTHS = {'u': 3641, 'g': 4704, 'r': 6155, 'i': 7504, 'z': 8695, 'y': 10056}
R_V = 3.1
FILTERS = ['u', 'g', 'r', 'i', 'z', 'y']

# 1. T·∫£i m√¥ h√¨nh ƒê√É HU·∫§N LUY·ªÜN M·ªöI
print("--- ƒêang t·∫£i m√¥ h√¨nh ELAsTiCC2 ---")
try:
    model_info = joblib.load('optimized_tde_model_elastricc.pkl')  # ƒê·ªîI T√äN FILE
    print(f"‚úÖ ƒê√£ t·∫£i model t·ª´: 'optimized_tde_model_elastricc.pkl'")
except FileNotFoundError:
    print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y model m·ªõi, th·ª≠ t·∫£i model c≈©...")
    model_info = joblib.load('optimized_tde_model.pkl')

selected_features = model_info['features']
best_threshold = model_info['best_threshold']
models = model_info['models']

print(f"Lo·∫°i m√¥ h√¨nh: {model_info['strategy']}")
print(f"S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng: {len(selected_features)}")
print(f"Ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.3f}")
print(f"S·ªë m√¥ h√¨nh: {len(models)}")
print(f"OOF F1 score: {model_info.get('oof_score', 'N/A'):.4f}")
print(f"Precision: {model_info.get('precision', 'N/A'):.4f}")
print(f"Recall: {model_info.get('recall', 'N/A'):.4f}")

# 2. T·∫£i metadata c·ªßa t·∫≠p test
print("\n--- ƒêang t·∫£i d·ªØ li·ªáu t·∫≠p test ---")
test_log = pd.read_csv(f'{BASE_PATH}test_log.csv')
print(f"S·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng trong t·∫≠p test: {len(test_log)}")

# 3. TH√äM: C√°c h√†m ELAsTiCC2 features cho test set
def calculate_color_features_test(df):
    """T√≠nh color features cho test set (gi·ªëng train)"""
    color_features_list = []
    
    for object_id in df['object_id'].unique():
        obj_data = df[df['object_id'] == object_id]
        
        features = {'object_id': object_id}
        
        # T√≠nh mean color gi·ªØa c√°c bands
        for band_pair in [('g', 'r'), ('r', 'i'), ('g', 'i'), ('u', 'g'), ('i', 'z'), ('z', 'y')]:
            band1, band2 = band_pair
            
            # Check if we have flux for both bands
            band1_mean_col = f'{band1}_mean'
            band2_mean_col = f'{band2}_mean'
            
            if band1_mean_col in obj_data.columns and band2_mean_col in obj_data.columns:
                band1_mean = obj_data[band1_mean_col].iloc[0] if not pd.isna(obj_data[band1_mean_col].iloc[0]) else np.nan
                band2_mean = obj_data[band2_mean_col].iloc[0] if not pd.isna(obj_data[band2_mean_col].iloc[0]) else np.nan
                
                if not np.isnan(band1_mean) and not np.isnan(band2_mean):
                    features[f'color_{band1}_{band2}_mean'] = band1_mean - band2_mean
        
        color_features_list.append(features)
    
    return pd.DataFrame(color_features_list)

def calculate_rise_fade_features_test(df):
    """T√≠nh rise v√† fade times cho test set"""
    features_list = []
    
    for object_id in df['object_id'].unique():
        obj_data = df[df['object_id'] == object_id].sort_values('mjd')
        
        if len(obj_data) < 5:
            features_list.append({'object_id': object_id})
            continue
            
        flux = obj_data['flux_corrected'].values
        mjd = obj_data['mjd'].values
        
        # T√¨m peak
        peak_idx = np.argmax(flux)
        
        # Rise time
        rise_time = mjd[peak_idx] - mjd[0] if peak_idx > 0 else np.nan
        
        # Fade time
        fade_time = mjd[-1] - mjd[peak_idx] if peak_idx < len(flux)-1 else np.nan
        
        # Rise rate
        if rise_time > 0 and peak_idx > 0:
            rise_rate = (flux[peak_idx] - flux[0]) / rise_time
        else:
            rise_rate = np.nan
            
        # Fade rate
        if fade_time > 0 and peak_idx < len(flux)-1:
            fade_rate = (flux[peak_idx] - flux[-1]) / fade_time
        else:
            fade_rate = np.nan
        
        features_list.append({
            'object_id': object_id,
            'rise_time': rise_time,
            'fade_time': fade_time,
            'rise_fade_ratio': rise_time / (fade_time + 1e-9),
            'rise_rate': rise_rate,
            'fade_rate': fade_rate,
            'total_duration': mjd[-1] - mjd[0]
        })
    
    return pd.DataFrame(features_list)

def enhanced_peak_analysis_test(group):
    """Ph√¢n t√≠ch peak n√¢ng cao cho test set"""
    group = group.sort_values('mjd')
    flux = group['flux_corrected'].values
    
    features = {}
    
    # T√¨m peaks
    peaks, properties = find_peaks(flux, prominence=0.1, width=1)
    
    features['peak_count'] = len(peaks)
    
    if len(peaks) > 0:
        # Peak properties
        features['peak_prominence_mean'] = np.mean(properties['prominences'])
        features['peak_prominence_max'] = np.max(properties['prominences'])
        
        # Main peak analysis
        main_peak_idx = peaks[np.argmax(properties['prominences'])]
        features['main_peak_flux'] = flux[main_peak_idx]
        
        # Peak symmetry
        if main_peak_idx > 0 and main_peak_idx < len(flux)-1:
            left_side = flux[:main_peak_idx]
            right_side = flux[main_peak_idx+1:]
            
            if len(left_side) > 0 and len(right_side) > 0:
                features['peak_symmetry'] = np.mean(left_side) / (np.mean(right_side) + 1e-9)
                features['peak_asymmetry'] = abs(features['peak_symmetry'] - 1)
    
    # Peak-to-median ratio
    features['peak_to_median'] = np.max(flux) / (np.median(flux) + 1e-9)
    features['peak_to_mean'] = np.max(flux) / (np.mean(flux) + 1e-9)
    
    return pd.Series(features)

def calculate_temporal_features_test(group):
    """T√≠nh c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian cho test set"""
    group = group.sort_values('mjd')
    mjd = group['mjd'].values
    flux = group['flux_corrected'].values
    
    features = {}
    
    if len(mjd) > 1:
        # Time gaps statistics
        gaps = np.diff(mjd)
        features['gap_mean'] = np.mean(gaps)
        features['gap_std'] = np.std(gaps)
        features['gap_max'] = np.max(gaps)
        
        # Observation density
        total_time = mjd[-1] - mjd[0]
        features['obs_density'] = len(mjd) / (total_time + 1e-9)
    
    # Time series autocorrelation
    if len(flux) >= 5:
        autocorr = np.correlate(flux - np.mean(flux), flux - np.mean(flux), mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        
        # Lags 1-3
        for lag in [1, 2, 3]:
            if lag < len(autocorr):
                features[f'autocorr_lag{lag}'] = autocorr[lag] / autocorr[0] if autocorr[0] != 0 else 0
    
    return pd.Series(features)

def calculate_advanced_statistical_features_test(flux):
    """T√≠nh c√°c ƒë·∫∑c tr∆∞ng th·ªëng k√™ n√¢ng cao cho test set"""
    features = {}
    
    if len(flux) < 3:
        return features
    
    # Percentile ratios
    percentiles = [10, 25, 50, 75, 90, 95]
    percentile_values = np.percentile(flux, percentiles)
    
    # Important ratios
    if len(percentile_values) >= 5:
        features['p90_p10_ratio'] = percentile_values[4] / (percentile_values[0] + 1e-9)
        features['p75_p25_ratio'] = percentile_values[3] / (percentile_values[1] + 1e-9)
        features['p95_p5_ratio'] = percentile_values[5] / (np.percentile(flux, 5) + 1e-9) if len(percentile_values) >= 6 else np.nan
    
    # IQR
    q75, q25 = np.percentile(flux, [75, 25])
    features['iqr'] = q75 - q25
    
    return features

def extract_improved_features_test(df):
    """Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng c·∫£i ti·∫øn cho test set"""
    features_list = []
    
    for object_id in df['object_id'].unique():
        obj_data = df[df['object_id'] == object_id].sort_values('mjd')
        
        features = {}
        flux = obj_data['flux_corrected'].values
        mjd = obj_data['mjd'].values
        
        # 1. X·ª≠ l√Ω flux √¢m
        flux_positive = flux.clip(min=1e-9)
        flux_abs = np.abs(flux)
        
        # ƒê·∫∑c tr∆∞ng theo ph√¢n v·ªã
        for q in [10, 25, 50, 75, 90]:
            features[f'flux_q{q}'] = np.percentile(flux_positive, q)
            features[f'flux_abs_q{q}'] = np.percentile(flux_abs, q)
        
        features['positive_ratio'] = (flux > 0).mean()
        features['flux_abs_median'] = np.median(flux_abs)
        
        # 2. ƒê·∫∑c tr∆∞ng kho·∫£ng tr·ªëng quan s√°t
        if len(mjd) > 1:
            mjd_diff = np.diff(mjd)
            features['gap_mean'] = np.mean(mjd_diff)
            features['gap_std'] = np.std(mjd_diff)
            features['gap_max'] = np.max(mjd_diff)
            features['gap_median'] = np.median(mjd_diff)
            features['obs_density'] = len(obj_data) / (mjd[-1] - mjd[0] + 1e-9)
        
        # 3. ƒê·∫∑c tr∆∞ng theo d·∫£i s√≥ng
        unique_filters = obj_data['Filter'].nunique()
        features['filter_coverage'] = unique_filters / 6.0
        
        # T·ª∑ l·ªá s·ªë l·∫ßn quan s√°t theo t·ª´ng d·∫£i
        for band in ['u', 'g', 'r', 'i', 'z', 'y']:
            band_count = (obj_data['Filter'] == band).sum()
            features[f'{band}_obs_ratio'] = band_count / len(obj_data)
        
        # 4. ƒê·∫∑c tr∆∞ng t√≠n hi·ªáu tr√™n nhi·ªÖu
        snr = flux_positive / (obj_data['flux_err'].values + 1e-9)
        features['snr_median'] = np.median(snr)
        features['snr_q10'] = np.percentile(snr, 10)
        features['snr_q90'] = np.percentile(snr, 90)
        
        # 5. ƒê·ªô ·ªïn ƒë·ªãnh xu h∆∞·ªõng (Theil-Sen)
        if len(flux) >= 5:
            try:
                x = np.arange(len(flux))
                slope, _, _, _ = theilslopes(flux, x)
                features['trend_slope'] = slope
                predicted = slope * x + np.median(flux)
                residuals = flux - predicted
                features['trend_stability'] = 1.0 / (np.std(residuals) / (np.std(flux) + 1e-9) + 1e-9)
            except:
                features['trend_slope'] = 0
                features['trend_stability'] = 0
        else:
            features['trend_slope'] = 0
            features['trend_stability'] = 0
        
        # 6. ƒê·∫∑c tr∆∞ng h√¨nh th√°i
        if len(flux) >= 3:
            features['variability_index'] = np.std(flux) / (np.mean(flux_positive) + 1e-9)
            features['peakiness'] = np.max(flux_positive) / (np.median(flux_positive) + 1e-9)
            
            peak_idx = np.argmax(flux)
            if 0 < peak_idx < len(flux) - 1:
                before_peak = flux[:peak_idx]
                after_peak = flux[peak_idx+1:]
                std_before = np.std(before_peak) if len(before_peak) > 1 else 0
                std_after = np.std(after_peak) if len(after_peak) > 1 else 0
                features['asymmetry'] = std_after / (std_before + 1e-9)
            else:
                features['asymmetry'] = 0
            
            if len(flux) > 5:
                first_half = flux[:len(flux)//2]
                second_half = flux[len(flux)//2:]
                features['rise_fall_ratio_global'] = np.mean(first_half) / (np.mean(second_half) + 1e-9)
        
        # 7. Th√™m advanced statistical features
        advanced_stats = calculate_advanced_statistical_features_test(flux)
        features.update(advanced_stats)
        
        features['object_id'] = object_id
        features_list.append(features)
    
    return pd.DataFrame(features_list)

# 4. C√°c h√†m g·ªëc (gi·ªØ nguy√™n nh∆∞ng th√™m cho test)
def tde_decay_model(t, alpha, t0, A):
    return A * (t - t0 + 1e-6) ** (-alpha)

def fit_decay_alpha(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == len(f) - 1: return np.nan
    t_decay = t[peak_idx:] - t[peak_idx]
    f_decay = f[peak_idx:]
    try:
        popt, _ = curve_fit(
            tde_decay_model, t_decay, f_decay,
            p0=[1.5, 0, f[peak_idx]],
            bounds=([0.5, -10, 0], [3.0, 10, np.inf]),
            maxfev=500
        )
        alpha = popt[0]
        return alpha if 0.5 <= alpha <= 3.0 else np.nan
    except:
        return np.nan

def is_single_peak(group):
    f = group['flux_corrected'].values
    if len(f) < 3: return 0
    peaks = (f[1:-1] > f[:-2]) & (f[1:-1] > f[2:])
    return int(np.sum(peaks) == 1)

def rise_fall_ratio(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == 0 or peak_idx == len(f) - 1: return np.nan
    rise_time = t[peak_idx] - t[0]
    fall_time = t[-1] - t[peak_idx]
    return rise_time / (fall_time + 1e-6)

def weighted_slope(group):
    """ƒê·ªô d·ªëc c√≥ tr·ªçng s·ªë theo th·ªùi gian"""
    group = group.sort_values('mjd')
    if len(group) < 2:
        return 0
    delta_mjd = np.diff(group['mjd'])
    delta_flux = np.diff(group['flux_corrected'])
    slopes = delta_flux / (delta_mjd + 1e-9)
    return np.mean(slopes) if len(slopes) > 0 else 0

# 5. X·ª≠ l√Ω light curve c·ªßa t·∫≠p test V·ªöI ELAsTiCC2 FEATURES
def process_test_lightcurves_elastricc():
    """X·ª≠ l√Ω light curve t·∫≠p test v·ªõi ELAsTiCC2 features"""
    
    print("\n--- ƒêang x·ª≠ l√Ω light curve t·∫≠p test v·ªõi ELAsTiCC2 features ---")
    
    # Ki·ªÉm tra c·∫•u tr√∫c file t·∫≠p test
    test_lc_paths = []
    
    # Ki·ªÉm tra file test ƒë∆°n
    single_test_path = f'{BASE_PATH}test_lightcurves/test_full_lightcurves.csv'
    if os.path.exists(single_test_path):
        test_lc_paths = [single_test_path]
        print("ƒê√£ t√¨m th·∫•y file test ƒë∆°n")
    else:
        # Ki·ªÉm tra c√°c file test chia nh·ªè
        for i in range(1, 21):
            split_path = f'{BASE_PATH}split_{i:02d}/test_full_lightcurves.csv'
            if os.path.exists(split_path):
                test_lc_paths.append(split_path)
        print(f"T√¨m th·∫•y {len(test_lc_paths)} file test ph√¢n m·∫£nh")
    
    if not test_lc_paths:
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y file light curve c·ªßa t·∫≠p test")
    
    all_test_features = []
    
    for lc_path in tqdm(test_lc_paths, desc="X·ª≠ l√Ω c√°c ph√¢n m·∫£nh test"):
        # T·∫£i d·ªØ li·ªáu light curve
        lc = pd.read_csv(lc_path)
        lc.rename(columns={'Time (MJD)': 'mjd', 'Flux': 'flux', 'Flux_err': 'flux_err'}, inplace=True)
        
        # Ch·ªâ x·ª≠ l√Ω object_id t·ªìn t·∫°i trong test_log
        relevant_objects = test_log['object_id'].unique()
        lc = lc[lc['object_id'].isin(relevant_objects)]
        
        if lc.empty:
            continue
        
        processed_dfs = []
        
        # Hi·ªáu ch·ªânh flux cho t·ª´ng object
        for object_id in lc['object_id'].unique():
            object_lc = lc[lc['object_id'] == object_id].copy()
            if object_lc.empty:
                continue
                
            # L·∫•y gi√° tr·ªã EBV c·ªßa object
            object_ebv = test_log[test_log['object_id'] == object_id]['EBV'].iloc[0]
            A_v = R_V * object_ebv
            
            flux_corrected_list = []
            for _, row in object_lc.iterrows():
                A_lambda = extinction.fitzpatrick99(
                    np.array([EFF_WAVELENGTHS[row['Filter']]]), A_v, R_V
                )[0]
                flux_corrected_list.append(row['flux'] * 10**(0.4 * A_lambda))
            
            object_lc['flux_corrected'] = flux_corrected_list
            processed_dfs.append(object_lc)
        
        if not processed_dfs:
            continue
            
        df = pd.concat(processed_dfs)
        
        # T√≠nh kho·∫£ng c√°ch v√† ƒë·ªô s√°ng tuy·ªát ƒë·ªëi
        df['Z'] = df['object_id'].map(test_log.set_index('object_id')['Z'])
        z_values = df['Z'].fillna(0).values
        z_values[z_values <= 0] = 1e-6
        
        # T√≠nh kho·∫£ng c√°ch
        dist_pc = cosmo.luminosity_distance(z_values).to(u.pc).value
        df['distance_pc'] = dist_pc
        
        # T√≠nh ƒë·ªô s√°ng tuy·ªát ƒë·ªëi
        df['flux_positive'] = df['flux_corrected'].clip(lower=1e-9)
        df['apparent_mag'] = -2.5 * np.log10(df['flux_positive'])
        df['absolute_mag'] = df['apparent_mag'] - 5 * (np.log10(df['distance_pc']) - 1)
        
        # T√≠nh to√°n c∆° b·∫£n
        df['snr'] = df['flux_corrected'] / df['flux_err']
        df = df.sort_values(['object_id', 'mjd'])
        
        # FEATURE ENGINEERING V·ªöI ELAsTiCC2
        grouped = df.groupby('object_id')
        
        # A. ƒê·∫∑c tr∆∞ng to√†n c·ª•c c∆° b·∫£n
        agg_features = grouped.agg(
            flux_mean=('flux_corrected', 'mean'),
            flux_std=('flux_corrected', 'std'),
            flux_max=('flux_corrected', 'max'),
            flux_min=('flux_corrected', 'min'),
            flux_skew=('flux_corrected', 'skew'),
            mjd_span=('mjd', lambda x: x.max() - x.min()),
            snr_mean=('snr', 'mean'),
            snr_max=('snr', 'max'),
            snr_std=('snr', 'std'),
            obs_count=('mjd', 'size')
        )
        
        # B. ƒê·∫∑c tr∆∞ng ƒë·ªô s√°ng tuy·ªát ƒë·ªëi
        abs_mag_agg = grouped.agg(
            abs_mag_min=('absolute_mag', 'min'),
            abs_mag_max=('absolute_mag', 'max'),
            abs_mag_mean=('absolute_mag', 'mean'),
            abs_mag_std=('absolute_mag', 'std'),
            abs_mag_span=('absolute_mag', lambda x: x.max() - x.min())
        )
        agg_features = agg_features.join(abs_mag_agg, how='left')
        
        # C. ƒê·∫∑c tr∆∞ng v·∫≠t l√Ω c∆° b·∫£n
        agg_features['weighted_slope_mean'] = grouped.apply(weighted_slope)
        agg_features['decay_alpha'] = grouped.apply(fit_decay_alpha)
        agg_features['is_single_peak'] = grouped.apply(is_single_peak)
        agg_features['rise_fall_ratio'] = grouped.apply(rise_fall_ratio)
        
        # D. ƒê·∫∑c tr∆∞ng theo d·∫£i s√≥ng
        pivot_mean = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='mean').add_suffix('_mean')
        pivot_std = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='std').add_suffix('_std')
        pivot_max = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='max').add_suffix('_max')
        agg_features = agg_features.join([pivot_mean, pivot_std, pivot_max], how='left')
        
        # E. ƒê·∫∑c tr∆∞ng m√†u s·∫Øc c∆° b·∫£n
        for f in FILTERS:
            if f'{f}_mean' not in agg_features.columns:
                agg_features[f'{f}_mean'] = np.nan
        
        agg_features['color_g_r_mean'] = agg_features['g_mean'] - agg_features['r_mean']
        agg_features['color_r_i_mean'] = agg_features['r_mean'] - agg_features['i_mean']
        agg_features['color_i_z_mean'] = agg_features['i_mean'] - agg_features['z_mean']
        
        # F. ƒê·∫∑c tr∆∞ng c·∫£i ti·∫øn g·ªëc
        improved_features = extract_improved_features_test(df)
        agg_features = agg_features.merge(improved_features, on='object_id', how='left')
        
        # G. ƒê·∫∂C TR∆ØNG ELAsTiCC2 M·ªöI
        # 1. Color features
        color_features = calculate_color_features_test(agg_features)
        agg_features = agg_features.merge(color_features, on='object_id', how='left')
        
        # 2. Rise/Fade features
        rise_fade_features = calculate_rise_fade_features_test(df)
        agg_features = agg_features.merge(rise_fade_features, on='object_id', how='left')
        
        # 3. Peak analysis features
        peak_features = grouped.apply(enhanced_peak_analysis_test)
        peak_features = peak_features.reset_index()
        agg_features = agg_features.merge(peak_features, on='object_id', how='left')
        
        # 4. Temporal features
        temporal_features = grouped.apply(calculate_temporal_features_test)
        temporal_features = temporal_features.reset_index()
        agg_features = agg_features.merge(temporal_features, on='object_id', how='left')
        
        all_test_features.append(agg_features)
    
    if not all_test_features:
        raise ValueError("Kh√¥ng t·∫°o ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng cho t·∫≠p test")
    
    # G·ªôp t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng
    test_feature_df = pd.concat(all_test_features)
    test_feature_df.reset_index(inplace=True)
    
    return test_feature_df

# 6. Sinh ƒë·∫∑c tr∆∞ng cho t·∫≠p test V·ªöI ELAsTiCC2
test_features_df = process_test_lightcurves_elastricc()

# 7. G·ªôp metadata v√† ƒë·∫∑c tr∆∞ng
print("\n--- G·ªôp d·ªØ li·ªáu t·∫≠p test ---")
test_final = test_log.merge(test_features_df, on='object_id', how='left')

# 8. KI·ªÇM TRA V√Ä X·ª¨ L√ù FEATURES
print("\n--- Ki·ªÉm tra features ---")

# Ki·ªÉm tra ƒë·∫∑c tr∆∞ng b·ªã thi·∫øu
missing_features = set(selected_features) - set(test_final.columns)
if missing_features:
    print(f"‚ö†Ô∏è  C·∫£nh b√°o: thi·∫øu {len(missing_features)} ƒë·∫∑c tr∆∞ng, s·∫Ω ƒëi·ªÅn 0")
    for feature in missing_features:
        test_final[feature] = 0

# Ki·ªÉm tra features th·ª´a (kh√¥ng c√≥ trong selected_features)
extra_features = set(test_final.columns) - set(selected_features + ['object_id', 'Z', 'EBV', 'distance_pc'])
print(f"C√≥ {len(extra_features)} features th·ª´a s·∫Ω b·ªã b·ªè qua")

# 9. Chu·∫©n b·ªã d·ªØ li·ªáu d·ª± ƒëo√°n
X_test = test_final[selected_features].copy()

# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu THEO CHI·∫æN L∆Ø·ª¢C C·ª¶A TRAIN
print("\n--- X·ª≠ l√Ω missing values ---")

# Ph√¢n lo·∫°i columns ƒë·ªÉ x·ª≠ l√Ω kh√°c nhau
std_cols = [c for c in X_test.columns if 'std' in c.lower()]
mean_cols = [c for c in X_test.columns if 'mean' in c.lower() and 'color' not in c.lower()]
color_cols = [c for c in X_test.columns if 'color' in c.lower()]
ratio_cols = [c for c in X_test.columns if 'ratio' in c.lower()]

# Fill missing values v·ªõi chi·∫øn l∆∞·ª£c gi·ªëng train
X_test[std_cols] = X_test[std_cols].fillna(0)
X_test[mean_cols] = X_test[mean_cols].fillna(X_test[mean_cols].median())
X_test[color_cols] = X_test[color_cols].fillna(0)
X_test[ratio_cols] = X_test[ratio_cols].fillna(1)

# Fill remaining numeric columns
numeric_cols = X_test.select_dtypes(include=[np.number]).columns
for col in numeric_cols:
    if X_test[col].isnull().any():
        if col.startswith('p') and col[1:].isdigit():
            X_test[col] = X_test[col].fillna(X_test[col].median())
        else:
            X_test[col] = X_test[col].fillna(0)

print(f"K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng t·∫≠p test: {X_test.shape}")
print(f"S·ªë l∆∞·ª£ng objects: {len(X_test)}")

# 10. Ti·∫øn h√†nh d·ª± ƒëo√°n
print("\n--- ƒêang d·ª± ƒëo√°n ---")
test_predictions = []

for i, model in enumerate(models):
    try:
        pred = model.predict_proba(X_test)[:, 1]
        test_predictions.append(pred)
        print(f"‚úÖ M√¥ h√¨nh {i+1}/{len(models)} ƒë√£ d·ª± ƒëo√°n xong")
    except Exception as e:
        print(f"‚ùå L·ªói v·ªõi m√¥ h√¨nh {i+1}: {e}")
        # N·∫øu c√≥ l·ªói, d√πng random predictions
        test_predictions.append(np.random.rand(len(X_test)) * 0.5)

# Trung b√¨nh d·ª± ƒëo√°n
test_preds_mean = np.mean(test_predictions, axis=0)

# √Åp d·ª•ng ng∆∞·ª°ng t·ªëi ∆∞u
test_binary = (test_preds_mean >= best_threshold).astype(int)

print(f"\nüéØ D·ª± ƒëo√°n t·∫≠p test ho√†n t·∫•t:")
print(f"   X√°c su·∫•t d·ª± ƒëo√°n trung b√¨nh: {test_preds_mean.mean():.4f} ¬± {test_preds_mean.std():.4f}")
print(f"   T·ª∑ l·ªá m·∫´u d∆∞∆°ng ƒë∆∞·ª£c d·ª± ƒëo√°n: {test_binary.mean():.4f} ({test_binary.sum()}/{len(test_binary)})")

print(f"\nüìä Ph√¢n b·ªë x√°c su·∫•t d·ª± ƒëo√°n:")
for percentile in [10, 25, 50, 75, 90]:
    value = np.percentile(test_preds_mean, percentile)
    print(f"   Ph√¢n v·ªã {percentile:2d}%: {value:.4f}")

# 11. T·∫°o file submission
print("\n--- T·∫°o file submission ---")
submission = pd.DataFrame({
    'object_id': test_final['object_id'],
    'predicted': test_binary
})

# ƒê·∫£m b·∫£o m·ªçi object trong t·∫≠p test ƒë·ªÅu c√≥ d·ª± ƒëo√°n
missing_objects = set(test_log['object_id']) - set(submission['object_id'])
if missing_objects:
    print(f"‚ö†Ô∏è  C·∫£nh b√°o: {len(missing_objects)} object ch∆∞a c√≥ d·ª± ƒëo√°n, g√°n 0")
    missing_df = pd.DataFrame({
        'object_id': list(missing_objects),
        'predicted': 0
    })
    submission = pd.concat([submission, missing_df], ignore_index=True)

# S·∫Øp x·∫øp theo object_id
submission = submission.sort_values('object_id')

# L∆∞u file submission V·ªöI T√äN M·ªöI
submission_file = 'submission_elastricc.csv'
submission.to_csv(submission_file, index=False)

print(f"\n‚úÖ ƒê√£ t·∫°o file submission: '{submission_file}'")
print(f"   K√≠ch th∆∞·ªõc file submission: {submission.shape}")
print(f"   S·ªë m·∫´u d∆∞∆°ng ƒë∆∞·ª£c d·ª± ƒëo√°n: {submission['predicted'].sum()}")
print(f"   T·ª∑ l·ªá m·∫´u d∆∞∆°ng: {submission['predicted'].mean():.4f}")

# 12. L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n chi ti·∫øt
detailed_predictions = pd.DataFrame({
    'object_id': test_final['object_id'],
    'prediction_prob': test_preds_mean,
    'predicted': test_binary,
    'Z': test_final['Z'],
    'EBV': test_final['EBV']
})
detailed_predictions.to_csv('test_detailed_predictions_elastricc.csv', index=False)
print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n chi ti·∫øt: 'test_detailed_predictions_elastricc.csv'")

# 13. Ph√¢n t√≠ch d·ª± ƒëo√°n N√ÇNG CAO
print("\n--- Ph√¢n t√≠ch d·ª± ƒëo√°n n√¢ng cao ---")
print(f"üìà Ng∆∞·ª°ng s·ª≠ d·ª•ng: {best_threshold:.3f}")
print(f"üìä Kho·∫£ng x√°c su·∫•t d·ª± ƒëo√°n: [{test_preds_mean.min():.4f}, {test_preds_mean.max():.4f}]")

# Ph√¢n lo·∫°i confidence levels
high_confidence_pos = (test_preds_mean > 0.7).sum()
medium_confidence_pos = ((test_preds_mean >= 0.5) & (test_preds_mean <= 0.7)).sum()
low_confidence_pos = ((test_preds_mean >= best_threshold) & (test_preds_mean < 0.5)).sum()

high_confidence_neg = (test_preds_mean < 0.3).sum()
medium_confidence_neg = ((test_preds_mean >= 0.3) & (test_preds_mean < best_threshold)).sum()

print(f"\nüéØ Confidence Analysis:")
print(f"   High confidence positives (p > 0.7): {high_confidence_pos}")
print(f"   Medium confidence positives (0.5 ‚â§ p ‚â§ 0.7): {medium_confidence_pos}")
print(f"   Low confidence positives ({best_threshold:.2f} ‚â§ p < 0.5): {low_confidence_pos}")
print(f"   High confidence negatives (p < 0.3): {high_confidence_neg}")
print(f"   Medium confidence negatives (0.3 ‚â§ p < {best_threshold:.2f}): {medium_confidence_neg}")

# 14. Feature importance analysis
print("\n--- Ph√¢n t√≠ch features tr√™n t·∫≠p test ---")
if hasattr(models[0], 'feature_importances_'):
    try:
        feature_importance = np.mean([model.feature_importances_ for model in models], axis=0)
        importance_df = pd.DataFrame({
            'feature': selected_features,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print("\nüèÜ Top 15 ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t tr√™n t·∫≠p test:")
        for i, row in importance_df.head(15).iterrows():
            print(f"   {row['importance']:.4f} - {row['feature']}")
        
        # L∆∞u feature importance
        importance_df.to_csv('test_feature_importance_elastricc.csv', index=False)
        
        # Hi·ªÉn th·ªã top ELAsTiCC2 features
        elastricc_features = [f for f in importance_df['feature'] if any(keyword in f for keyword in [
            'color_', 'rise_', 'fade_', 'peak_', 'autocorr_', 'p90_', 'p75_', 'p95_'
        ])]
        
        print(f"\nüî¨ Top ELAsTiCC2 features trong predictions:")
        for feat in elastricc_features[:10]:
            imp = importance_df[importance_df['feature'] == feat]['importance'].iloc[0]
            print(f"   {imp:.4f} - {feat}")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t√≠nh feature importance: {e}")

# 15. D·ª± ƒëo√°n LB score
print("\n--- D·ª± ƒëo√°n ƒëi·ªÉm LB ---")
oof_f1 = model_info.get('oof_score', 0)
if oof_f1 >= 0.45:
    print("üéâ XU·∫§T S·∫ÆC! D·ª± ki·∫øn LB score: 0.45+")
    print("   - Model v·ªõi ELAsTiCC2 features ho·∫°t ƒë·ªông t·ªët")
    print("   - C√≥ kh·∫£ nƒÉng ƒë·∫°t top leaderboard")
elif oof_f1 >= 0.40:
    print("üëç T·ªêT! D·ª± ki·∫øn LB score: 0.40-0.45")
    print("   - C·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi baseline")
    print("   - ELAsTiCC2 features c√≥ t√°c d·ª•ng t√≠ch c·ª±c")
elif oof_f1 >= 0.35:
    print("‚ö†Ô∏è  TRUNG B√åNH! D·ª± ki·∫øn LB score: 0.35-0.40")
    print("   - C·∫ßn xem x√©t distribution shift gi·ªØa train-test")
else:
    print("‚ùå C·∫¶N C·∫¢I THI·ªÜN! D·ª± ki·∫øn LB score: <0.35")
    print("   - C√≥ th·ªÉ c√≥ v·∫•n ƒë·ªÅ v·ªõi feature engineering")

print("\n" + "="*80)
print("üéâ HO√ÄN T·∫§T D·ª∞ ƒêO√ÅN T·∫¨P TEST V·ªöI ELAsTiCC2 MODEL!")
print("="*80)
print(f"üì§ H√£y n·ªôp file '{submission_file}' l√™n Kaggle")
print(f"üìä ƒêi·ªÉm OOF F1 c·ªßa model: {oof_f1:.4f}")
print(f"üéØ Ng∆∞·ª°ng t·ªëi ∆∞u: {best_threshold:.3f}")
print(f"üìà S·ªë l∆∞·ª£ng TDE d·ª± ƒëo√°n: {submission['predicted'].sum()}")
print(f"üîç T·ª∑ l·ªá TDE d·ª± ƒëo√°n: {submission['predicted'].mean():.4f}")

print("\nüí° L∆ØU √ù QUAN TR·ªåNG:")
print("1. Submission file: 'submission_elastricc.csv'")
print("2. Chi ti·∫øt predictions: 'test_detailed_predictions_elastricc.csv'")
print("3. So s√°nh v·ªõi submission c≈© ƒë·ªÉ ƒë√°nh gi√° c·∫£i thi·ªán")
print("4. Ki·ªÉm tra distribution c·ªßa features n·∫øu LB score th·∫•p")

print("\nüöÄ NEXT STEPS:")
print("1. Submit l√™n Kaggle: 'submission_elastricc.csv'")
print("2. So s√°nh score v·ªõi baseline")
print("3. Ph√¢n t√≠ch c√°c predictions c√≥ confidence cao")
print("4. N·∫øu c·∫ßn, fine-tune threshold d·ª±a tr√™n public LB")