import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import joblib
import warnings
warnings.filterwarnings('ignore')

print("=== Tinh chá»‰nh chi tiáº¿t mÃ´ hÃ¬nh phÃ¢n loáº¡i TDE ===")

# 1. Táº£i dá»¯ liá»‡u
print("--- Äang táº£i dá»¯ liá»‡u ---")
df = pd.read_csv('processed_train_features_improved.csv')
features_to_exclude = ['object_id', 'SpecType', 'English Translation', 'split', 'target']
all_features = [col for col in df.columns if col not in features_to_exclude]
X = df[all_features]
y = df['target']
groups = df['split']

print(f"KÃ­ch thÆ°á»›c dá»¯ liá»‡u: {X.shape}")
print(f"Tá»· lá»‡ máº«u dÆ°Æ¡ng: {y.mean():.4f}")

# 2. Chá»n táº­p con Ä‘áº·c trÆ°ng tá»‘i Æ°u dá»±a trÃªn káº¿t quáº£ trÆ°á»›c Ä‘Ã³
def select_optimal_features(df):
    """Chá»n Ä‘áº·c trÆ°ng tá»‘i Æ°u dá»±a trÃªn cÃ¡c thÃ­ nghiá»‡m trÆ°á»›c"""
    
    # CÃ¡c Ä‘áº·c trÆ°ng cÃ³ má»©c Ä‘á»™ quan trá»ng cao trong cÃ¡c káº¿t quáº£ trÆ°á»›c
    high_importance_features = [
        'flux_abs_q25', 'flux_abs_q10', 'decay_alpha', 'r_max', 'mjd_span',
        'Z', 'flux_abs_q50', 'u_mean', 'i_max', 'u_max', 'peakiness',
        'color_r_i_mean', 'g_max', 'abs_mag_mean', 'abs_mag_min',
        'abs_mag_max', 'rise_fall_ratio', 'rise_fall_ratio_global',
        'asymmetry', 'u_std', 'positive_ratio', 'flux_abs_median',
        'variability_index', 'trend_stability', 'filter_coverage',
        'gap_max', 'obs_density', 'snr_mean', 'obs_count', 'flux_mean'
    ]
    
    # Chá»‰ chá»n cÃ¡c Ä‘áº·c trÆ°ng tá»“n táº¡i trong dá»¯ liá»‡u
    selected = [f for f in high_importance_features if f in df.columns]
    print(f"ÄÃ£ chá»n {len(selected)} Ä‘áº·c trÆ°ng quan trá»ng cao")
    return selected

print("\n--- Lá»±a chá»n Ä‘áº·c trÆ°ng ---")
selected_features = select_optimal_features(df)
X_optimal = df[selected_features]

# 3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i nhiá»u chiáº¿n lÆ°á»£c
def train_multiple_strategies(X, y, groups):
    """Thá»­ nghiá»‡m nhiá»u chiáº¿n lÆ°á»£c huáº¥n luyá»‡n"""
    
    strategies = {}
    pos_weight = len(y) / (2 * np.sum(y))
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    # Chiáº¿n lÆ°á»£c 1: ChÃ­nh quy hÃ³a má»©c trung bÃ¬nh
    print("\n--- Chiáº¿n lÆ°á»£c 1: ChÃ­nh quy hÃ³a trung bÃ¬nh ---")
    models1, oof1, scores1 = train_strategy(X, y, groups, {
        'n_estimators': 1500,
        'learning_rate': 0.05,
        'num_leaves': 31,
        'max_depth': 7,
        'min_child_samples': 20,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.3,
        'reg_lambda': 0.5,
        'scale_pos_weight': pos_weight
    }, "ChÃ­nh quy hÃ³a trung bÃ¬nh")
    
    strategies['medium_reg'] = {
        'models': models1, 
        'oof_preds': oof1, 
        'scores': scores1,
        'params': 'ChÃ­nh quy hÃ³a trung bÃ¬nh'
    }
    
    # Chiáº¿n lÆ°á»£c 2: ChÃ­nh quy hÃ³a nháº¹ + dá»«ng sá»›m nghiÃªm ngáº·t
    print("\n--- Chiáº¿n lÆ°á»£c 2: ChÃ­nh quy hÃ³a nháº¹ + dá»«ng sá»›m nghiÃªm ngáº·t ---")
    models2, oof2, scores2 = train_strategy(X, y, groups, {
        'n_estimators': 1000,
        'learning_rate': 0.1,
        'num_leaves': 63,
        'max_depth': 8,
        'min_child_samples': 10,
        'subsample': 0.9,
        'colsample_bytree': 0.9,
        'reg_alpha': 0.1,
        'reg_lambda': 0.2,
        'scale_pos_weight': pos_weight
    }, "ChÃ­nh quy hÃ³a nháº¹", early_stopping_rounds=50)
    
    strategies['light_reg'] = {
        'models': models2, 
        'oof_preds': oof2, 
        'scores': scores2,
        'params': 'ChÃ­nh quy hÃ³a nháº¹'
    }
    
    # Chiáº¿n lÆ°á»£c 3: Há»c táº­p tá»• há»£p (LightGBM + RandomForest)
    print("\n--- Chiáº¿n lÆ°á»£c 3: Tá»• há»£p mÃ´ hÃ¬nh ---")
    ensemble_preds = np.zeros(len(X))
    ensemble_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"Huáº¥n luyá»‡n tá»• há»£p - Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # LightGBM
        lgb_model = lgb.LGBMClassifier(
            n_estimators=500,
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            random_state=42 + fold,
            verbosity=-1
        )
        lgb_model.fit(X_train, y_train)
        lgb_preds = lgb_model.predict_proba(X_val)[:, 1]
        
        # RandomForest
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42 + fold,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        rf_preds = rf_model.predict_proba(X_val)[:, 1]
        
        # Dá»± Ä‘oÃ¡n trung bÃ¬nh
        ensemble_pred = (lgb_preds + rf_preds) / 2
        ensemble_preds[val_idx] = ensemble_pred
        
        # TÃ­nh F1 cho fold hiá»‡n táº¡i
        best_f1_fold = 0
        for thresh in np.arange(0.1, 0.9, 0.02):
            f1 = f1_score(y_val, (ensemble_pred >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
        
        ensemble_scores.append(best_f1_fold)
        print(f"Fold {fold + 1} F1: {best_f1_fold:.4f}")
    
    strategies['ensemble'] = {
        'models': None,  # MÃ´ hÃ¬nh tá»• há»£p khÃ´ng lÆ°u tá»«ng mÃ´ hÃ¬nh riÃªng láº»
        'oof_preds': ensemble_preds, 
        'scores': ensemble_scores,
        'params': 'Tá»• há»£p LightGBM + RF'
    }
    
    return strategies

def train_strategy(X, y, groups, params, strategy_name, early_stopping_rounds=100):
    """Huáº¥n luyá»‡n má»™t chiáº¿n lÆ°á»£c Ä‘Æ¡n láº»"""
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    models = []
    oof_preds = np.zeros(len(X))
    fold_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"{strategy_name} - Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = lgb.LGBMClassifier(**params, verbosity=-1, n_jobs=-1)
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            eval_metric='binary_logloss',
            callbacks=[
                lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False),
                lgb.log_evaluation(period=0)
            ]
        )
        
        val_preds = model.predict_proba(X_val)[:, 1]
        oof_preds[val_idx] = val_preds
        
        # TÃ¬m ngÆ°á»¡ng tá»‘i Æ°u
        best_f1_fold = 0
        for thresh in np.arange(0.1, 0.9, 0.02):
            f1 = f1_score(y_val, (val_preds >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
        
        fold_scores.append(best_f1_fold)
        models.append(model)
        print(f"Fold {fold + 1} F1: {best_f1_fold:.4f}")
    
    return models, oof_preds, fold_scores

# 4. ÄÃ¡nh giÃ¡ táº¥t cáº£ chiáº¿n lÆ°á»£c
def evaluate_strategies(strategies, y):
    """ÄÃ¡nh giÃ¡ toÃ n bá»™ chiáº¿n lÆ°á»£c huáº¥n luyá»‡n"""
    
    results = []
    best_strategy = None
    best_f1 = 0
    
    for name, strategy in strategies.items():
        oof_preds = strategy['oof_preds']
        
        # Tá»‘i Æ°u ngÆ°á»¡ng
        best_threshold = 0.5
        best_f1_score = 0
        for thresh in np.arange(0.1, 0.9, 0.01):
            f1 = f1_score(y, (oof_preds >= thresh).astype(int))
            if f1 > best_f1_score:
                best_f1_score = f1
                best_threshold = thresh
        
        # TÃ­nh cÃ¡c chá»‰ sá»‘ khÃ¡c
        binary_preds = (oof_preds >= best_threshold).astype(int)
        precision = np.mean(y[binary_preds == 1]) if np.sum(binary_preds) > 0 else 0
        recall = np.mean(binary_preds[y == 1])
        
        # PhÃ¢n tÃ­ch quÃ¡ khá»›p (chá»‰ Ã¡p dá»¥ng cho chiáº¿n lÆ°á»£c mÃ´ hÃ¬nh Ä‘Æ¡n)
        if strategy['models'] is not None:
            train_f1 = analyze_overfitting(strategy['models'], X_optimal, y, best_threshold)
            overfitting_gap = train_f1 - best_f1_score
        else:
            train_f1 = np.nan
            overfitting_gap = np.nan
        
        results.append({
            'strategy': name,
            'params': strategy['params'],
            'oof_f1': best_f1_score,
            'precision': precision,
            'recall': recall,
            'threshold': best_threshold,
            'train_f1': train_f1,
            'overfitting_gap': overfitting_gap,
            'fold_scores': strategy['scores']
        })
        
        if best_f1_score > best_f1:
            best_f1 = best_f1_score
            best_strategy = name
    
    return pd.DataFrame(results), best_strategy

def analyze_overfitting(models, X, y, threshold):
    """PhÃ¢n tÃ­ch hiá»‡n tÆ°á»£ng quÃ¡ khá»›p"""
    train_preds = []
    for model in models:
        train_pred = model.predict_proba(X)[:, 1]
        train_preds.append(train_pred)
    
    train_preds_mean = np.mean(train_preds, axis=0)
    return f1_score(y, (train_preds_mean >= threshold).astype(int))

# 5. Quy trÃ¬nh huáº¥n luyá»‡n chÃ­nh
print("\n--- Báº¯t Ä‘áº§u huáº¥n luyá»‡n Ä‘a chiáº¿n lÆ°á»£c ---")
strategies = train_multiple_strategies(X_optimal, y, groups)

print("\n--- ÄÃ¡nh giÃ¡ chiáº¿n lÆ°á»£c ---")
results_df, best_strategy_name = evaluate_strategies(strategies, y)

print("\n=== Káº¿t quáº£ táº¥t cáº£ chiáº¿n lÆ°á»£c ===")
for _, row in results_df.iterrows():
    print(f"\n{row['strategy']} ({row['params']}):")
    print(f"  OOF F1: {row['oof_f1']:.4f}")
    print(f"  Äá»™ chÃ­nh xÃ¡c: {row['precision']:.4f}")
    print(f"  Äá»™ bao phá»§ (Recall): {row['recall']:.4f}")
    print(f"  NgÆ°á»¡ng: {row['threshold']:.3f}")
    if not pd.isna(row['overfitting_gap']):
        print(f"  Khoáº£ng cÃ¡ch quÃ¡ khá»›p: {row['overfitting_gap']:.4f}")
    print(f"  F1 tá»«ng Fold: {[f'{s:.4f}' for s in row['fold_scores']]}")

print(f"\nğŸ¯ Chiáº¿n lÆ°á»£c tá»‘t nháº¥t: {best_strategy_name}")
best_strategy = strategies[best_strategy_name]

# 6. Tá»‘i Æ°u mÃ´ hÃ¬nh cuá»‘i cÃ¹ng
print("\n--- Tá»‘i Æ°u mÃ´ hÃ¬nh cuá»‘i cÃ¹ng ---")

# Náº¿u chiáº¿n lÆ°á»£c tá»‘t nháº¥t lÃ  tá»• há»£p, cáº§n huáº¥n luyá»‡n phiÃªn báº£n cÃ³ thá»ƒ lÆ°u
if best_strategy_name == 'ensemble':
    print("Chiáº¿n lÆ°á»£c tá»‘t nháº¥t lÃ  tá»• há»£p, Ä‘ang huáº¥n luyá»‡n phiÃªn báº£n cÃ³ thá»ƒ lÆ°u...")
    
    final_models = []
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_optimal, y, groups)):
        print(f"Huáº¥n luyá»‡n cuá»‘i cÃ¹ng - Fold {fold + 1}")
        
        X_train, X_val = X_optimal.iloc[train_idx], X_optimal.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = lgb.LGBMClassifier(
            n_estimators=1000,
            learning_rate= 0.05 if best_strategy_name == 'medium_reg' else 0.1,
            num_leaves= 31 if best_strategy_name == 'medium_reg' else 63,
            max_depth= 7 if best_strategy_name == 'medium_reg' else 8,
            min_child_samples= 20 if best_strategy_name == 'medium_reg' else 10,
            subsample= 0.8 if best_strategy_name == 'medium_reg' else 0.9,
            colsample_bytree= 0.8 if best_strategy_name == 'medium_reg' else 0.9,
            reg_alpha= 0.3 if best_strategy_name == 'medium_reg' else 0.1,
            reg_lambda= 0.5 if best_strategy_name == 'medium_reg' else 0.2,
            scale_pos_weight= len(y) / (2 * np.sum(y)),
            random_state=42 + fold,
            verbosity=-1,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train)
        final_models.append(model)
    
    best_models = final_models
else:
    best_models = best_strategy['models']

# 7. LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t
print("\n--- LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t ---")
best_model_info = {
    'models': best_models,
    'features': selected_features,
    'best_threshold': results_df[results_df['strategy'] == best_strategy_name]['threshold'].iloc[0],
    'oof_score': results_df[results_df['strategy'] == best_strategy_name]['oof_f1'].iloc[0],
    'strategy': best_strategy_name,
    'feature_names': selected_features
}

joblib.dump(best_model_info, 'optimized_tde_model.pkl')

# LÆ°u káº¿t quáº£ chi tiáº¿t
results_df.to_csv('training_strategies_comparison.csv', index=False)

# LÆ°u dá»± Ä‘oÃ¡n OOF
best_oof_preds = best_strategy['oof_preds']
oof_results = pd.DataFrame({
    'object_id': df['object_id'],
    'true_target': y,
    'oof_prediction': best_oof_preds,
    'oof_binary': (best_oof_preds >= best_model_info['best_threshold']).astype(int)
})
oof_results.to_csv('optimized_oof_predictions.csv', index=False)

print("âœ… HoÃ n táº¥t tinh chá»‰nh!")
print(f"ğŸ“Š Chiáº¿n lÆ°á»£c tá»‘t nháº¥t: {best_strategy_name}")
print(f"ğŸ¯ OOF F1 tá»‘t nháº¥t: {best_model_info['oof_score']:.4f}")
print(f"âš–ï¸  NgÆ°á»¡ng tá»‘i Æ°u: {best_model_info['best_threshold']:.3f}")
print(f"ğŸ“ MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u: 'optimized_tde_model.pkl'")
print(f"ğŸ“ So sÃ¡nh chiáº¿n lÆ°á»£c Ä‘Ã£ lÆ°u: 'training_strategies_comparison.csv'")

# 8. Nháº­n Ä‘á»‹nh then chá»‘t
print("\n--- Nháº­n Ä‘á»‹nh then chá»‘t ---")
print("PhÃ¢n tÃ­ch váº¥n Ä‘á» hiá»‡n táº¡i:")
print("1. QuÃ¡ khá»›p nghiÃªm trá»ng - cáº§n cÃ¢n báº±ng giá»¯a chÃ­nh quy hÃ³a vÃ  Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh")
print("2. Máº¥t cÃ¢n báº±ng dá»¯ liá»‡u - chá»‰ cÃ³ 148 máº«u dÆ°Æ¡ng, cáº§n chiáº¿n lÆ°á»£c láº¥y máº«u tá»‘t hÆ¡n")
print("3. Cháº¥t lÆ°á»£ng Ä‘áº·c trÆ°ng - cáº§n Ä‘áº£m báº£o Ä‘áº·c trÆ°ng cÃ³ kháº£ nÄƒng phÃ¢n biá»‡t")

print("\nÄá» xuáº¥t bÆ°á»›c tiáº¿p theo:")
if best_model_info['oof_score'] < 0.38:
    print("âš ï¸  OOF F1 váº«n cÃ²n tháº¥p, Ä‘á» xuáº¥t:")
    print("   - Kiá»ƒm tra láº¡i quy trÃ¬nh táº¡o Ä‘áº·c trÆ°ng")
    print("   - Thá»­ ká»¹ thuáº­t oversampling (SMOTE)")
    print("   - CÃ¢n nháº¯c sá»­ dá»¥ng máº¡ng nÆ¡-ron")
else:
    print("âœ… Káº¿t quáº£ cháº¥p nháº­n Ä‘Æ°á»£c, cÃ³ thá»ƒ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n trÃªn táº­p test")
