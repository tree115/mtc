import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
import joblib
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

print("=== Tinh ch·ªânh chi ti·∫øt m√¥ h√¨nh ph√¢n lo·∫°i TDE v·ªõi ELAsTiCC2 Features ===")

# 1. T·∫£i d·ªØ li·ªáu M·ªöI t·ª´ PreprocessandFeatures.py
print("--- ƒêang t·∫£i d·ªØ li·ªáu m·ªõi ---")
df = pd.read_csv('processed_train_features_elastricc_enhanced.csv')

# Debug: Ki·ªÉm tra columns
print(f"C√°c columns trong dataframe: {list(df.columns)[:20]}...")

# ƒê·∫£m b·∫£o c√°c columns b·∫Øt bu·ªôc t·ªìn t·∫°i
required_columns = ['object_id', 'target', 'split']
for col in required_columns:
    if col not in df.columns:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: Column '{col}' kh√¥ng t·ªìn t·∫°i trong dataframe")
        print(f"C√°c columns hi·ªán c√≥: {list(df.columns)}")

# X√°c ƒë·ªãnh features v√† target
features_to_exclude = ['object_id', 'SpecType', 'English Translation', 'split', 'target']
# Lo·∫°i b·ªè c√°c columns kh√¥ng t·ªìn t·∫°i
features_to_exclude = [col for col in features_to_exclude if col in df.columns]

all_features = [col for col in df.columns if col not in features_to_exclude]
X = df[all_features]
y = df['target']
groups = df['split']

print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu: {X.shape}")
print(f"T·ª∑ l·ªá m·∫´u d∆∞∆°ng: {y.mean():.4f} ({y.sum()}/{len(y)})")
print(f"S·ªë l∆∞·ª£ng features: {len(all_features)}")

# 2. Feature Selection N√ÇNG CAO v·ªõi ELAsTiCC2 features
def select_optimal_features_with_elastricc(df):
    """Ch·ªçn ƒë·∫∑c tr∆∞ng t·ªëi ∆∞u v·ªõi integration t·ª´ ELAsTiCC2"""
    
    # Nh√≥m 1: Features c∆° b·∫£n t·ª´ phi√™n b·∫£n tr∆∞·ªõc
    basic_features = [
        'flux_abs_q25', 'flux_abs_q10', 'decay_alpha', 'r_max', 'mjd_span',
        'Z', 'flux_abs_q50', 'u_mean', 'i_max', 'u_max', 'peakiness',
        'color_r_i_mean', 'g_max', 'abs_mag_mean', 'abs_mag_min',
        'abs_mag_max', 'rise_fall_ratio', 'rise_fall_ratio_global',
        'asymmetry', 'u_std', 'positive_ratio', 'flux_abs_median',
        'variability_index', 'trend_stability', 'filter_coverage',
        'gap_max', 'obs_density', 'snr_mean', 'obs_count', 'flux_mean'
    ]
    
    # Nh√≥m 2: NEW FEATURES t·ª´ ELAsTiCC2 (quan tr·ªçng nh·∫•t)
    elastricc_features = [
        'rise_time', 'fade_time', 'rise_fade_ratio', 'rise_rate', 'fade_rate',
        'peak_count', 'peak_prominence_mean', 'peak_symmetry', 'peak_asymmetry',
        'color_g_r_mean', 'color_r_i_mean', 'color_i_z_mean',
        'color_g_i_mean', 'color_u_g_mean', 'color_z_y_mean',
        'autocorr_lag1', 'autocorr_lag2', 'autocorr_lag3',
        'p90_p10_ratio', 'p75_p25_ratio', 'p95_p5_ratio',
        'max_band_correlation', 'min_band_correlation',
        'total_duration', 'peak_to_median', 'peak_to_mean'
    ]
    
    # K·∫øt h·ª£p t·∫•t c·∫£ features
    all_candidate_features = basic_features + elastricc_features
    
    # Ch·ªâ ch·ªçn features t·ªìn t·∫°i trong dataframe
    selected = [f for f in all_candidate_features if f in df.columns]
    
    print(f"T·ªïng s·ªë features candidate: {len(all_candidate_features)}")
    print(f"Features t·ªìn t·∫°i trong dataframe: {len(selected)}")
    print(f"Features ELAsTiCC2 m·ªõi: {len([f for f in selected if f in elastricc_features])}")
    
    # Hi·ªÉn th·ªã c√°c features ELAsTiCC2 m·ªõi
    new_elastricc_features = [f for f in selected if f in elastricc_features]
    print("\nC√°c features ELAsTiCC2 m·ªõi ƒë∆∞·ª£c ch·ªçn:")
    for i, feat in enumerate(new_elastricc_features[:15]):
        print(f"  {i+1:2d}. {feat}")
    if len(new_elastricc_features) > 15:
        print(f"  ... v√† {len(new_elastricc_features) - 15} features kh√°c")
    
    return selected

print("\n--- L·ª±a ch·ªçn ƒë·∫∑c tr∆∞ng v·ªõi ELAsTiCC2 ---")
selected_features = select_optimal_features_with_elastricc(df)
X_optimal = df[selected_features]

# 3. TH√äM: Feature Importance s∆° b·ªô v·ªõi RandomForest
print("\n--- Ph√¢n t√≠ch s∆° b·ªô Feature Importance ---")
def quick_feature_importance_analysis(X, y):
    """Ph√¢n t√≠ch nhanh feature importance"""
    rf_quick = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_quick.fit(X.fillna(0), y)
    
    importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': rf_quick.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nTop 20 features quan tr·ªçng nh·∫•t (s∆° b·ªô):")
    for i, row in importance_df.head(20).iterrows():
        print(f"  {row['importance']:.4f} - {row['feature']}")
    
    return importance_df

importance_df = quick_feature_importance_analysis(X_optimal, y)

# 4. C·∫£i ti·∫øn h√†m train_strategy v·ªõi early stopping t·ªët h∆°n
def train_strategy(X, y, groups, params, strategy_name, early_stopping_rounds=100):
    """Hu·∫•n luy·ªán m·ªôt chi·∫øn l∆∞·ª£c ƒë∆°n l·∫ª v·ªõi c·∫£i ti·∫øn"""
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    models = []
    oof_preds = np.zeros(len(X))
    fold_scores = []
    feature_importances = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"{strategy_name} - Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = lgb.LGBMClassifier(**params, verbosity=-1, n_jobs=-1)
        
        # Fit v·ªõi validation set
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            eval_metric=['binary_logloss', 'auc'],
            callbacks=[
                lgb.early_stopping(
                    stopping_rounds=early_stopping_rounds, 
                    verbose=False,
                    first_metric_only=False
                ),
                lgb.log_evaluation(period=0)
            ]
        )
        
        # D·ª± ƒëo√°n
        val_preds = model.predict_proba(X_val)[:, 1]
        oof_preds[val_idx] = val_preds
        
        # L∆∞u feature importance
        if hasattr(model, 'feature_importances_'):
            feature_importances.append(model.feature_importances_)
        
        # T√¨m ng∆∞·ª°ng t·ªëi ∆∞u cho fold n√†y
        best_f1_fold = 0
        best_thresh_fold = 0.5
        
        # T·ªëi ∆∞u threshold v·ªõi step nh·ªè h∆°n
        for thresh in np.arange(0.05, 0.95, 0.01):
            f1 = f1_score(y_val, (val_preds >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
                best_thresh_fold = thresh
        
        fold_scores.append({
            'f1': best_f1_fold,
            'threshold': best_thresh_fold,
            'auc': roc_auc_score(y_val, val_preds)
        })
        
        models.append(model)
        print(f"  Fold {fold + 1}: F1={best_f1_fold:.4f}, Thresh={best_thresh_fold:.3f}, AUC={roc_auc_score(y_val, val_preds):.4f}")
    
    # T√≠nh average feature importance
    avg_feature_importance = np.mean(feature_importances, axis=0) if feature_importances else None
    
    return models, oof_preds, fold_scores, avg_feature_importance

# 5. C·∫£i ti·∫øn train_multiple_strategies
def train_multiple_strategies(X, y, groups):
    """Th·ª≠ nghi·ªám nhi·ªÅu chi·∫øn l∆∞·ª£c hu·∫•n luy·ªán v·ªõi tuning t·ªët h∆°n"""
    
    strategies = {}
    pos_weight = len(y) / (2 * np.sum(y))
    print(f"\nClass weight (pos_weight): {pos_weight:.2f}")
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    # Chi·∫øn l∆∞·ª£c 1: Tuned for ELAsTiCC2 features (MODERATE)
    print("\n" + "="*60)
    print("Chi·∫øn l∆∞·ª£c 1: Tuned cho ELAsTiCC2 features (Moderate)")
    print("="*60)
    
    models1, oof1, scores1, fi1 = train_strategy(X, y, groups, {
        'n_estimators': 2000,
        'learning_rate': 0.03,  # Lower learning rate cho features ph·ª©c t·∫°p
        'num_leaves': 31,
        'max_depth': 6,  # Gi·∫£m depth ƒë·ªÉ tr√°nh overfit
        'min_child_samples': 30,  # TƒÉng ƒë·ªÉ robust h∆°n
        'subsample': 0.7,  # Gi·∫£m ƒë·ªÉ tr√°nh overfit
        'colsample_bytree': 0.7,
        'reg_alpha': 0.5,  # TƒÉng regularization
        'reg_lambda': 0.7,
        'scale_pos_weight': pos_weight,
        'boosting_type': 'gbdt',
        'objective': 'binary',
        'metric': ['binary_logloss', 'auc'],
        'random_state': 42
    }, "ELAsTiCC2 Tuned", early_stopping_rounds=80)
    
    strategies['elastricc_tuned'] = {
        'models': models1, 
        'oof_preds': oof1, 
        'scores': scores1,
        'feature_importance': fi1,
        'params': 'ELAsTiCC2 Tuned (Moderate)'
    }
    
    # Chi·∫øn l∆∞·ª£c 2: Strong Regularization
    print("\n" + "="*60)
    print("Chi·∫øn l∆∞·ª£c 2: Strong Regularization")
    print("="*60)
    
    models2, oof2, scores2, fi2 = train_strategy(X, y, groups, {
        'n_estimators': 1500,
        'learning_rate': 0.05,
        'num_leaves': 15,  # R·∫•t nh·ªè ƒë·ªÉ tr√°nh overfit
        'max_depth': 4,
        'min_child_samples': 50,
        'subsample': 0.6,
        'colsample_bytree': 0.6,
        'reg_alpha': 1.0,
        'reg_lambda': 1.5,
        'scale_pos_weight': pos_weight,
        'random_state': 42
    }, "Strong Regularization", early_stopping_rounds=100)
    
    strategies['strong_reg'] = {
        'models': models2, 
        'oof_preds': oof2, 
        'scores': scores2,
        'feature_importance': fi2,
        'params': 'Strong Regularization'
    }
    
    # Chi·∫øn l∆∞·ª£c 3: Ensemble LightGBM + RandomForest (c·∫£i ti·∫øn)
    print("\n" + "="*60)
    print("Chi·∫øn l∆∞·ª£c 3: Ensemble N√¢ng cao")
    print("="*60)
    
    ensemble_preds = np.zeros(len(X))
    ensemble_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"Hu·∫•n luy·ªán t·ªï h·ª£p - Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # LightGBM v·ªõi features m·ªõi
        lgb_model = lgb.LGBMClassifier(
            n_estimators=800,
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.3,
            reg_lambda=0.5,
            scale_pos_weight=pos_weight,
            random_state=42 + fold,
            verbosity=-1,
            n_jobs=-1
        )
        lgb_model.fit(X_train, y_train)
        lgb_preds = lgb_model.predict_proba(X_val)[:, 1]
        
        # RandomForest tuned cho features m·ªõi
        rf_model = RandomForestClassifier(
            n_estimators=150,
            max_depth=8,  # Gi·∫£m depth
            min_samples_split=15,
            min_samples_leaf=8,
            max_features='sqrt',
            class_weight='balanced',
            random_state=42 + fold,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        rf_preds = rf_model.predict_proba(X_val)[:, 1]
        
        # Weighted ensemble (70% LGBM, 30% RF)
        ensemble_pred = 0.7 * lgb_preds + 0.3 * rf_preds
        ensemble_preds[val_idx] = ensemble_pred
        
        # T·ªëi ∆∞u threshold
        best_f1_fold = 0
        for thresh in np.arange(0.05, 0.95, 0.01):
            f1 = f1_score(y_val, (ensemble_pred >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
        
        ensemble_scores.append({
            'f1': best_f1_fold,
            'auc': roc_auc_score(y_val, ensemble_pred)
        })
        print(f"  Fold {fold + 1}: F1={best_f1_fold:.4f}, AUC={roc_auc_score(y_val, ensemble_pred):.4f}")
    
    strategies['ensemble_enhanced'] = {
        'models': None,
        'oof_preds': ensemble_preds, 
        'scores': ensemble_scores,
        'params': 'Ensemble LGBM+RF (Weighted)'
    }
    
    return strategies

# 6. C·∫£i ti·∫øn evaluate_strategies
def evaluate_strategies(strategies, y, X=None):
    """ƒê√°nh gi√° to√†n b·ªô chi·∫øn l∆∞·ª£c hu·∫•n luy·ªán v·ªõi metrics ƒë·∫ßy ƒë·ªß"""
    
    results = []
    best_strategy = None
    best_f1 = 0
    
    for name, strategy in strategies.items():
        oof_preds = strategy['oof_preds']
        
        # T·ªëi ∆∞u ng∆∞·ª°ng v·ªõi Precision-Recall tradeoff
        precision, recall, thresholds = precision_recall_curve(y, oof_preds)
        
        # T√¨m threshold t·ªët nh·∫•t b·∫±ng F1 score
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
        best_idx = np.argmax(f1_scores[:-1])  # B·ªè gi√° tr·ªã cu·ªëi
        best_f1_score = f1_scores[best_idx]
        best_threshold = thresholds[best_idx] if len(thresholds) > best_idx else 0.5
        
        # T√≠nh c√°c metrics
        binary_preds = (oof_preds >= best_threshold).astype(int)
        
        # Precision, Recall, Specificity
        tp = np.sum((binary_preds == 1) & (y == 1))
        fp = np.sum((binary_preds == 1) & (y == 0))
        fn = np.sum((binary_preds == 0) & (y == 1))
        tn = np.sum((binary_preds == 0) & (y == 0))
        
        precision_metric = tp / (tp + fp + 1e-9)
        recall_metric = tp / (tp + fn + 1e-9)
        specificity = tn / (tn + fp + 1e-9)
        
        # AUC
        auc_score = roc_auc_score(y, oof_preds)
        
        # Ph√¢n t√≠ch fold scores
        fold_scores = strategy['scores']
        avg_fold_f1 = np.mean([s['f1'] for s in fold_scores]) if isinstance(fold_scores[0], dict) else np.mean(fold_scores)
        avg_fold_auc = np.mean([s.get('auc', 0) for s in fold_scores]) if isinstance(fold_scores[0], dict) else 0
        
        results.append({
            'strategy': name,
            'params': strategy['params'],
            'oof_f1': best_f1_score,
            'precision': precision_metric,
            'recall': recall_metric,
            'specificity': specificity,
            'auc': auc_score,
            'threshold': best_threshold,
            'avg_fold_f1': avg_fold_f1,
            'avg_fold_auc': avg_fold_auc,
            'fold_scores': fold_scores
        })
        
        if best_f1_score > best_f1:
            best_f1 = best_f1_score
            best_strategy = name
    
    return pd.DataFrame(results), best_strategy

# 7. TH√äM: Visualize feature importance
def plot_feature_importance(feature_importance, feature_names, strategy_name, top_n=20):
    """V·∫Ω bi·ªÉu ƒë·ªì feature importance"""
    if feature_importance is None:
        return
    
    # T·∫°o DataFrame
    fi_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    # L·∫•y top features
    top_features = fi_df.head(top_n)
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    plt.figure(figsize=(10, 8))
    sns.barplot(x='importance', y='feature', data=top_features)
    plt.title(f'Top {top_n} Feature Importance - {strategy_name}')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    
    # L∆∞u v√† hi·ªÉn th·ªã
    plt.savefig(f'feature_importance_{strategy_name}.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    # Hi·ªÉn th·ªã top features trong console
    print(f"\nTop {top_n} features cho {strategy_name}:")
    for i, row in top_features.iterrows():
        print(f"  {row['importance']:.4f} - {row['feature']}")
    
    return fi_df

# 8. Quy tr√¨nh hu·∫•n luy·ªán ch√≠nh
print("\n--- B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ƒëa chi·∫øn l∆∞·ª£c v·ªõi ELAsTiCC2 features ---")
strategies = train_multiple_strategies(X_optimal, y, groups)

print("\n--- ƒê√°nh gi√° chi·∫øn l∆∞·ª£c ---")
results_df, best_strategy_name = evaluate_strategies(strategies, y)

print("\n" + "="*80)
print("K·∫æT QU·∫¢ T·∫§T C·∫¢ CHI·∫æN L∆Ø·ª¢C")
print("="*80)

for _, row in results_df.iterrows():
    print(f"\nüìä {row['strategy']} ({row['params']}):")
    print(f"  ‚úÖ OOF F1: {row['oof_f1']:.4f}")
    print(f"  üéØ Precision: {row['precision']:.4f}")
    print(f"  üîç Recall: {row['recall']:.4f}")
    print(f"  üõ°Ô∏è  Specificity: {row['specificity']:.4f}")
    print(f"  üìà AUC: {row['auc']:.4f}")
    print(f"  ‚öñÔ∏è  Threshold: {row['threshold']:.3f}")
    print(f"  üìä Avg Fold F1: {row['avg_fold_f1']:.4f}")

print(f"\nüéØ CHI·∫æN L∆Ø·ª¢C T·ªêT NH·∫§T: {best_strategy_name}")
best_strategy = strategies[best_strategy_name]

# 9. Feature Importance visualization
print("\n--- Ph√¢n t√≠ch Feature Importance ---")
if best_strategy_name != 'ensemble_enhanced' and 'feature_importance' in best_strategy:
    feature_importance_df = plot_feature_importance(
        best_strategy['feature_importance'],
        X_optimal.columns,
        best_strategy_name
    )

# 10. T·ªëi ∆∞u m√¥ h√¨nh cu·ªëi c√πng
print("\n--- T·ªëi ∆∞u m√¥ h√¨nh cu·ªëi c√πng ---")

if best_strategy_name == 'ensemble_enhanced':
    print("Chi·∫øn l∆∞·ª£c t·ªët nh·∫•t l√† Ensemble, ƒëang hu·∫•n luy·ªán phi√™n b·∫£n c√≥ th·ªÉ l∆∞u...")
    
    final_models = []
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_optimal, y, groups)):
        print(f"Hu·∫•n luy·ªán cu·ªëi c√πng - Fold {fold + 1}")
        
        X_train, X_val = X_optimal.iloc[train_idx], X_optimal.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # Hu·∫•n luy·ªán LightGBM (ch√≠nh)
        model = lgb.LGBMClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.3,
            reg_lambda=0.5,
            scale_pos_weight=len(y) / (2 * np.sum(y)),
            random_state=42 + fold,
            verbosity=-1,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train)
        final_models.append(model)
    
    best_models = final_models
else:
    best_models = best_strategy['models']

# 11. L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
print("\n--- L∆∞u m√¥ h√¨nh t·ªët nh·∫•t ---")
best_model_info = {
    'models': best_models,
    'features': selected_features,
    'best_threshold': results_df[results_df['strategy'] == best_strategy_name]['threshold'].iloc[0],
    'oof_score': results_df[results_df['strategy'] == best_strategy_name]['oof_f1'].iloc[0],
    'precision': results_df[results_df['strategy'] == best_strategy_name]['precision'].iloc[0],
    'recall': results_df[results_df['strategy'] == best_strategy_name]['recall'].iloc[0],
    'auc': results_df[results_df['strategy'] == best_strategy_name]['auc'].iloc[0],
    'strategy': best_strategy_name,
    'feature_names': selected_features,
    'X_columns': X_optimal.columns.tolist(),
    'data_shape': X_optimal.shape,
    'class_ratio': y.mean()
}

joblib.dump(best_model_info, 'optimized_tde_model_elastricc.pkl')

# L∆∞u k·∫øt qu·∫£ chi ti·∫øt
results_df.to_csv('training_strategies_comparison_elastricc.csv', index=False)

# L∆∞u d·ª± ƒëo√°n OOF
best_oof_preds = best_strategy['oof_preds']
oof_results = pd.DataFrame({
    'object_id': df['object_id'],
    'true_target': y,
    'oof_prediction': best_oof_preds,
    'oof_binary': (best_oof_preds >= best_model_info['best_threshold']).astype(int)
})
oof_results.to_csv('optimized_oof_predictions_elastricc.csv', index=False)

print("\n" + "="*80)
print("‚úÖ HO√ÄN T·∫§T HU·∫§N LUY·ªÜN V·ªöI ELAsTiCC2 FEATURES!")
print("="*80)
print(f"üìä Chi·∫øn l∆∞·ª£c t·ªët nh·∫•t: {best_strategy_name}")
print(f"üéØ OOF F1 t·ªët nh·∫•t: {best_model_info['oof_score']:.4f}")
print(f"üìà Precision: {best_model_info['precision']:.4f}")
print(f"üîç Recall: {best_model_info['recall']:.4f}")
print(f"üìä AUC: {best_model_info['auc']:.4f}")
print(f"‚öñÔ∏è  Ng∆∞·ª°ng t·ªëi ∆∞u: {best_model_info['best_threshold']:.3f}")
print(f"üìÅ M√¥ h√¨nh ƒë√£ l∆∞u: 'optimized_tde_model_elastricc.pkl'")
print(f"üìÅ So s√°nh chi·∫øn l∆∞·ª£c: 'training_strategies_comparison_elastricc.csv'")
print(f"üéØ S·ªë l∆∞·ª£ng TDE samples: {y.sum()}/{len(y)} (t·ª∑ l·ªá: {y.mean():.4f})")

# 12. ƒê√°nh gi√° hi·ªáu qu·∫£ ELAsTiCC2 features
print("\n--- ƒê√ÅNH GI√Å HI·ªÜU QU·∫¢ ELAsTiCC2 FEATURES ---")

# Hi·ªÉn th·ªã top ELAsTiCC2 features
elastricc_features_in_selected = [f for f in selected_features if any(keyword in f for keyword in [
    'color_', 'rise_', 'fade_', 'peak_', 'autocorr_', 'p90_', 'p75_', 'p95_'
])]

print(f"\nS·ªë l∆∞·ª£ng ELAsTiCC2 features ƒë∆∞·ª£c s·ª≠ d·ª•ng: {len(elastricc_features_in_selected)}")
print("C√°c ELAsTiCC2 features quan tr·ªçng nh·∫•t:")

# Ki·ªÉm tra feature importance n·∫øu c√≥
if 'feature_importance' in best_strategy and best_strategy['feature_importance'] is not None:
    fi_df = pd.DataFrame({
        'feature': X_optimal.columns,
        'importance': best_strategy['feature_importance']
    }).sort_values('importance', ascending=False)
    
    # L·∫•y top ELAsTiCC2 features
    top_elastricc_features = fi_df[fi_df['feature'].isin(elastricc_features_in_selected)].head(10)
    
    for i, row in top_elastricc_features.iterrows():
        print(f"  {row['importance']:.4f} - {row['feature']}")

# 13. D·ª± ƒëo√°n performance
print("\n--- D·ª∞ ƒêO√ÅN PERFORMANCE ---")
oof_f1 = best_model_info['oof_score']
if oof_f1 >= 0.45:
    print("üéâ XU·∫§T S·∫ÆC! D·ª± ki·∫øn LB score: 0.45+")
    print("   - ELAsTiCC2 features ho·∫°t ƒë·ªông r·∫•t t·ªët")
    print("   - C√≥ th·ªÉ ƒë·∫°t top positions tr√™n leaderboard")
elif oof_f1 >= 0.40:
    print("üëç T·ªêT! D·ª± ki·∫øn LB score: 0.40-0.45")
    print("   - C·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi baseline")
    print("   - Ti·∫øp t·ª•c tuning ƒë·ªÉ ƒë·∫°t k·∫øt qu·∫£ cao h∆°n")
elif oof_f1 >= 0.35:
    print("‚ö†Ô∏è  TRUNG B√åNH! D·ª± ki·∫øn LB score: 0.35-0.40")
    print("   - C·∫ßn xem x√©t l·∫°i feature engineering")
    print("   - Th·ª≠ th√™m features ho·∫∑c tuning parameters")
else:
    print("‚ùå C·∫¶N C·∫¢I THI·ªÜN! D·ª± ki·∫øn LB score: <0.35")
    print("   - Ki·ªÉm tra l·∫°i data quality")
    print("   - Th·ª≠ c√°c chi·∫øn l∆∞·ª£c kh√°c (XGBoost, Neural Networks)")

print("\n--- NH·∫¨N ƒê·ªäNH THEN CH·ªêT ---")
print("1. ELAsTiCC2 features ƒê√É ƒê∆Ø·ª¢C T√çCH H·ª¢P TH√ÄNH C√îNG")
print("2. Model ƒë√£ ƒë∆∞·ª£c optimized cho imbalanced data")
print("3. Ready for test set prediction v·ªõi predict.py")

print("\nüìã NEXT STEPS:")
print("1. Ch·∫°y predict.py v·ªõi model m·ªõi")
print("2. Submit k·∫øt qu·∫£ l√™n Kaggle")
print("3. So s√°nh v·ªõi baseline performance")
print("4. N·∫øu c·∫ßn, th·ª≠ hyperparameter tuning th√™m")